---
title: "Regression Modeling Assignment: Question 1"
author: "Fazal Mahmud Niloy"
date: "16/10/2022"
output: html_document
---

```{r}
library(ggplot2)

usap <- read.csv("usap.csv") 
names(usap)
```
# Question 1

## a. Make Scatter Plots of the data

```{r}
plot(usap$gr, usap$vs, xlab="Vote Share", ylab="Growth Rate", main="Vote Share vs Growth Rate")
plot(usap$ir, usap$vs, xlab="Vote Share", ylab="Inflation Rate", main="Vote Share vs Inflation Rate", pch=19)
plot(usap$gn, usap$vs, xlab="Vote Share", ylab="Good News Quarters", main="Vote Share vs Good News Quarters", pch=19)
plot(usap$dv, usap$vs, xlab="Vote Share", ylab="Duration", main="Vote Share vs Duration", pch=19)
```

### b. Fit three possible regression models for Vote share

#### Model A: using Growth rate, Inflation rate

```{r}
modelA <- lm(vs ~ gr + ir, data=usap)
# plot(usap$vs, usap$gr + usap$ir, ylab="Model B", main="vs ~ gr + ir")
summary(modelA)
# make a multiple regression plot
plot(modelA, which=1)
```

#### Model B: using Growth rate, Inflation rate, Good news quarters and Duration value

```{r}
modelB <- lm(vs ~ gr + ir + gn + dv, data=usap)
# plot(usap$vs, usap$gr + usap$ir + usap$gn + usap$dv, xlab="Vote Share", ylab="Model B", main="vs ~ gr + ir + gn + dv")
summary(modelB)
plot(modelB, which = 1)
```

#### Model C: using Growth rate, Inflation rate, Good news quarters, Duration value, President running, Party, and President running \* Party which is the interaction between President running and Party

```{r}
modelC <- lm(vs ~ gr + ir + gn + dv + pr + pwh + pr*pwh, data=usap)
# plot(usap$vs, usap$gr + usap$ir + usap$gn + usap$dv + usap$pr + usap$pwh + usap$pr*usap$pwh, xlab="vote share", ylab="Growth Rate", main="vs ~ gr + ir + gn + dv + pr + pwh + pr*pwh")
summary(modelC)
plot(modelC, which = 1)
```

## c. Compare Models B and C, using the F-test with the restriction on the coefficients in the null hypothesis.

```{r}
anova(modelB, modelC)
```

From the ANOVA table we can see the F value is 3.9312 which says that the two models are significantly different. As the difference is big we would keep the better model. Now lets find the Critical value of the F distribution

Now lets find the Critical value of the F distribution:

```{r}
qf(0.05, 3, 14, lower.tail = FALSE)
```

since 3.9312 \> 3.343889, the Nested F-statistics falls in the rejection region, and we reject null hypothesis in favour of alternative Hypothesis.

#### Checking summary of modelB:

```{r}
summary(modelB)
```

#### Checking summary of modelC:

```{r}
summary(modelC)
```

As for modelC summary table ir, pr, ppr variables are non-significant and are not important for model.

From the ANOVA table we can see that alpha(0.03155) is less than 0.05, so we reject null hypothesis, thus we accept alternative hypothesis which defines at least one B5, B6, B7 is not equal to zero.

Conclusion: **modelC is better than modelB.**

## d. Compare the fitted models from b) and choose your best model
```{r}
smryA <- summary(modelA)
smryB <- summary(modelB)
smryC <- summary(modelC)
# make dataframes for the summary with the adjusted r-squared values
model_summary <- data.frame(model=c("A", "B", "C"), adj.r.squared=c(smryA$adj.r.squared, smryB$adj.r.squared, smryC$adj.r.squared))
model_summary[which(model_summary$adj.r.squared == max(model_summary$adj.r.squared)),]
```
Since we are using "adjusted r-squared values" to measure models we can say that **model C** is the best model among 3.

## e. Present a scatter plot of the standardised residuals against the fitted values
```{r}
ggplot(modelC, aes (x=fitted(modelC), y=rstandard(modelC))) +
  geom_point() +
  geom_line(aes(y=0), color="red") +
  ggtitle("Standardised Residuals vs Fitted Values") +
  xlab("Fitted Values") +
  ylab("Standardised Residuals")
```
From the Residual vs fitted values scatter-plot we can see,

1. From the scatter-plot we can see that there is no specific pattern of the points thus it satisfies the independence assumption.
2. The points look fairly evenly spread out, thus it satisfies constant variance assumption.
3. The relation is non-liner

## f. For your best model from b), examine if there are any outliers or influential observations
we can find the outliers by following code:
```{r}
k <- ols_prep_cdplot_data(modelC)
outlierChart <- ols_prep_outlier_obs(k)
outlierChart$obs[outlierChart$fct_color == "outlier"]
```
as we can see, **observation 2, 8, 9 & 20 are potential outliers**. We can also visualize the outliers by using the following code:
```{r}
ols_plot_cooksd_bar(modelC)
```

## g. Use your best model from b) to make point and interval estimates for A. Gore
```{r}
predGore <- data.frame(
  gr=usap$gr[22],
  ir=usap$ir[22],
  gn=usap$gn[22],
  dv=usap$dv[22],
  pr=usap$pr[22],
  pwh=usap$pwh[22],
  "pr:pwh"=usap$pr[22]*usap$pwh[22])
predict(modelC, predGore, interval="confidence")
predict(modelC, predGore, interval="prediction")[1,1]
```
from our observation we get vote share of **49.74993% which is close to the real value 50.3%**.

# Question 2
## a. Present a paragraph to describe what these methods are and why they are important, and what you should be aware of when you use such methods.
#### Linear Regression:
Linear regression is a statistical method for predicting a
quantitative response using a linear combination of
explanatory variables. In this model it tries to predict a
straight line where the response variables can be fitted and
then predicted after fitting.

#### Ridge Regression:
Ridge regression is a regression analysis method that estimates
the coefficients of a linear regression model with a
shrinkage penalty on the size of the coefficients.
Ridge regression is also known as Tikhonov regularization. It can
work great to generalize a model.

#### Lasso Regression:
Lasso regression is a regression analysis method that estimates
sparse coefficients. It is also known as least absolute
shrinkage and selection operator regression. The absolute
value of slope is added as a penalty term.
It can be used to select important variables in a model.

#### Elastic Net Regression:
Elastic net regression is a regression analysis method that
combines the penalties of both ridge regression and lasso
regression.

#### KNN Regression:
KNN regression is a regression analysis method that uses
the K nearest neighbors of a data point to predict the value
of the data point. It can be used to classify data points.

## b. Describe the "Hitters" datset and its relevant background information.

This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.

It is a data frame with 322 observations of major league players on the following 20 variables:

**AtBat**: Number of times at bat in 1986

**Hits**: Number of hits in 1986

**HmRun**: Number of home runs in 1986

**Runs**: Number of runs in 1986

**RBI**: Number of runs batted in in 1986

**Walks**: Number of walks in 1986

**Years**: Number of years in the major leagues

**CAtBat**: Number of times at bat during his career

**CHits**: Number of hits during his career

**CHmRun**: Number of home runs during his career

**CRuns**: Number of runs during his career

**CRBI**: Number of runs batted in during his career

**CWalks**: Number of walks during his career

**League**: A factor with levels A and N indicating player's league at the end of 1986

**Division**: A factor with levels E and W indicating player's division at the end of 1986

**PutOuts**: Number of put outs in 1986

**Assists**: Number of assists in 1986

**Errors**: Number of errors in 1986

**Salary**: 1987 annual salary on opening day in thousands of dollars

**NewLeague**: A factor with levels A and N indicating player's league at the beginning of 1987
```{r}
library(tidyverse)
library(glmnet)
library(caret)
library(ISLR)


data(Hitters, package = "ISLR")
# creating a new df to strore library data
hitters <- Hitters
head(hitters, 3)
names(hitters)
```

## c. Remove the missing values. Present a histogram and boxplot for the response variable salary and examine if there are any outliers.
#### Removing NA values
We are finding the number of NA values in the dataset.
```{r}
sum(is.na(hitters))
sum(is.na(hitters$Salary))
```
We can see there are *59* NA values in the dataset. Since all of the missing values are in the **Salary** column, we replace the NA values with the mean values (because these values are numeric)
```{r}
hitters$Salary[which(is.na(hitters$Salary))] <- mean(hitters$Salary, na.rm = TRUE)
sum(is.na(hitters$Salary))
max(hitters$Salary)
min(hitters$Salary)
```

#### Histogram of Salary
```{r}
hist(hitters$Salary, main="Salary", xlab="Salary")
```
#### Boxplot of Salary
```{r}
boxplot(hitters$Salary, main="Salary", xlab="Salary")
```